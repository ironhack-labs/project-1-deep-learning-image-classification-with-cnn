{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Data Preprocessing**\n",
    "   - Data loading and preprocessing (e.g., normalization, resizing, augmentation).\n",
    "   - Create visualizations of some images, and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Preprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "# Check if GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"GPU Devices: \", tf.config.experimental.list_physical_devices('GPU'))\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "\n",
    "# Cargar CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar muestras\n",
    "num_classes = 10\n",
    "samples_per_class = 10\n",
    "fig, axes = plt.subplots(num_classes, samples_per_class, figsize=(15, 15))\n",
    "\n",
    "for i in range(num_classes):\n",
    "    class_indices = np.where(y_train == i)[0]\n",
    "    random_indices = np.random.choice(class_indices, samples_per_class, replace=False)\n",
    "    for j, idx in enumerate(random_indices):\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(x_train[idx])\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "# Increase Data Augmentation\n",
    "\"\"\"\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.2\n",
    ")\n",
    "datagen.fit(x_train)\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "datagen.fit(x_train)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Model Architecture**\n",
    "   - Design a CNN architecture suitable for image classification.\n",
    "   - Include convolutional layers, pooling layers, and fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more Dropout layers and L2 Regularization\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# Add Early Stopping and Reduce Learning Rate on Plateau\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5)\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(64, (3,3), padding='same', activation='relu', input_shape=(32,32,3), kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3,3), padding='same', activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Conv2D(128, (3,3), padding='same', activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(128, (3,3), padding='same', activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Conv2D(256, (3,3), padding='same', activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(256, (3,3), padding='same', activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Use Learning Rate Scheduler\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 75:\n",
    "        lr = 0.0005\n",
    "    if epoch > 100:\n",
    "        lr = 0.0003\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer,\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Fit the model with the new data generator and learning rate scheduler\n",
    "\"\"\"\n",
    "history = model.fit(datagen.flow(x_train, y_train, batch_size=64),\n",
    "                   epochs=150,\n",
    "                   validation_data=(x_test, y_test),\n",
    "                   callbacks=[early_stopping, reduce_lr, lr_scheduler])\n",
    "\"\"\"\n",
    "history = model.fit(x_train, y_train, batch_size=512,\n",
    "                   epochs=150,\n",
    "                   validation_data=(x_test, y_test),\n",
    "                   callbacks=[early_stopping, reduce_lr, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "# Predicciones\n",
    "predictions = model.predict(x_test)\n",
    "predictions_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Métricas\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(confusion_matrix(true_classes, predictions_classes))\n",
    "\n",
    "print(\"\\nMétricas de Evaluación:\")\n",
    "print(f\"F1-Score: {f1_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "print(f\"Precisión: {precision_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "print(f\"Recall: {recall_score(true_classes, predictions_classes, average='weighted'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model2 = Sequential([\n",
    "    Conv2D(64, (3,3), padding='same', activation='relu', input_shape=(32,32,3)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Conv2D(128, (3,3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(128, (3,3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(128, (3,3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Conv2D(256, (3,3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(256, (3,3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(256, (3,3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Model Training**\n",
    "   - Train the CNN model using appropriate optimization techniques (e.g., stochastic gradient descent, Adam).\n",
    "   - Utilize techniques such as early stopping to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model2.compile(optimizer=optimizer,\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = model2.fit(x_train, y_train,\n",
    "                   batch_size=32,  # Change this value to any of the suggested batch sizes\n",
    "                   epochs=200,\n",
    "                   validation_split=0.2,\n",
    "                   callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Model Evaluation**\n",
    "   - Evaluate the trained model on a separate validation set.\n",
    "   - Compute and report metrics such as accuracy, precision, recall, and F1-score.\n",
    "   - Visualize the confusion matrix to understand model performance across different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "# Predicciones\n",
    "predictions = model2.predict(x_test)\n",
    "predictions_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Asegurarse de que las clases estén en el rango correcto (0-9 para CIFAR-10)\n",
    "print(\"Rango de predicciones:\", np.min(predictions_classes), \"-\", np.max(predictions_classes))\n",
    "print(\"Rango de valores reales:\", np.min(true_classes), \"-\", np.max(true_classes))\n",
    "\n",
    "# Calcular métricas solo si los rangos son correctos\n",
    "if (0 <= np.min(predictions_classes) <= 9) and (0 <= np.max(predictions_classes) <= 9):\n",
    "    print(\"\\nMatriz de Confusión:\")\n",
    "    print(confusion_matrix(true_classes, predictions_classes))\n",
    "\n",
    "    print(\"\\nMétricas de Evaluación:\")\n",
    "    print(f\"F1-Score: {f1_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "    print(f\"Precisión: {precision_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "    print(f\"Recall: {recall_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "else:\n",
    "    print(\"Error: Las predicciones no están en el rango esperado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Transfer Learning**\n",
    "    - Evaluate the accuracy of your model on a pre-trained models like ImagNet, VGG16, Inception... (pick one an justify your choice)\n",
    "        - You may find this [link](https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub) helpful.\n",
    "        - [This](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) is the Pytorch version.\n",
    "    - Perform transfer learning with your chosen pre-trained models i.e., you will probably try a few and choose the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "# Cargar VGG16 pre-entrenado\n",
    "base_model = VGG16(weights='imagenet', \n",
    "                   include_top=False, \n",
    "                   input_shape=(32, 32, 3))\n",
    "                   #classifier_activation=\"SoftMax\"))\n",
    "\n",
    "# Congelar capas base\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Crear modelo de transfer learning\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "transfer_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Entrenar modelo de transfer learning\n",
    "transfer_model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "history_transfer = transfer_model.fit(x_train, \n",
    "                                    y_train,\n",
    "                                    batch_size=128,\n",
    "                                    epochs=50,\n",
    "                                    validation_data=(x_test, y_test),\n",
    "                                    callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Evaluar métricas después del transfer learning\n",
    "predictions = transfer_model.predict(x_test)\n",
    "predictions_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"\\nMétricas después del transfer learning:\")\n",
    "print(f\"F1-Score: {f1_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "print(f\"Precisión: {precision_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "print(f\"Recall: {recall_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning\n",
    "for layer in base_model.layers[-4:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "transfer_model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "history_fine_tuning = transfer_model.fit(x_train, \n",
    "                                       y_train,\n",
    "                                       batch_size=128,\n",
    "                                       epochs=30,\n",
    "                                       validation_data=(x_test, y_test),\n",
    "                                       callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Evaluar métricas después del fine-tuning\n",
    "predictions = transfer_model.predict(x_test)\n",
    "predictions_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"\\nMétricas después del fine-tuning:\")\n",
    "print(f\"F1-Score: {f1_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "print(f\"Precisión: {precision_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "print(f\"Recall: {recall_score(true_classes, predictions_classes, average='weighted'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "# Predicciones\n",
    "predictions = transfer_model.predict(x_test)  # Nota: cambiado de base_model a transfer_model\n",
    "predictions_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Asegurarse de que las clases estén en el rango correcto (0-9 para CIFAR-10)\n",
    "print(\"Rango de predicciones:\", np.min(predictions_classes), \"-\", np.max(predictions_classes))\n",
    "print(\"Rango de valores reales:\", np.min(true_classes), \"-\", np.max(true_classes))\n",
    "\n",
    "# Calcular métricas solo si los rangos son correctos\n",
    "if (0 <= np.min(predictions_classes) <= 9) and (0 <= np.max(predictions_classes) <= 9):\n",
    "    print(\"\\nMatriz de Confusión:\")\n",
    "    print(confusion_matrix(true_classes, predictions_classes))\n",
    "\n",
    "    print(\"\\nMétricas de Evaluación:\")\n",
    "    print(f\"F1-Score: {f1_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "    print(f\"Precisión: {precision_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "    print(f\"Recall: {recall_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "else:\n",
    "    print(\"Error: Las predicciones no están en el rango esperado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Code Quality**\n",
    "   - Well-structured and commented code.\n",
    "   - Proper documentation of functions and processes.\n",
    "   - Efficient use of libraries and resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Report**\n",
    "   - Write a concise report detailing the approach taken, including:\n",
    "     - Description of the chosen CNN architecture.\n",
    "     - Explanation of preprocessing steps.\n",
    "     - Details of the training process (e.g., learning rate, batch size, number of epochs).\n",
    "     - Results and analysis of models performance.\n",
    "     - What is your best model. Why?\n",
    "     - Insights gained from the experimentation process.\n",
    "   - Include visualizations and diagrams where necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Model deployment**\n",
    "     - Pick the best model \n",
    "     - Build an app using Flask - Can you host somewhere other than your laptop? **+5 Bonus points if you use [Tensorflow Serving](https://www.tensorflow.org/tfx/guide/serving)**\n",
    "     - User should be able to upload one or multiples images get predictions including probabilities for each prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el mejor modelo (el que tenga mejor rendimiento entre el base y transfer learning)\n",
    "model.save('best_model.keras')\n",
    "\n",
    "# Crear aplicación Flask\n",
    "from flask import Flask, request, jsonify\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "app = Flask(__name__)\n",
    "model = load_model('best_model.keras')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    file = request.files['image']\n",
    "    img = Image.open(file)\n",
    "    img = img.resize((32, 32))\n",
    "    img_array = np.array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    pred = model.predict(img_array)\n",
    "    class_idx = np.argmax(pred[0])\n",
    "    confidence = float(pred[0][class_idx])\n",
    "    \n",
    "    return jsonify({\n",
    "        'class': int(class_idx),\n",
    "        'confidence': confidence\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
