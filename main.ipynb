{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Data Preprocessing**\n",
    "   - Data loading and preprocessing (e.g., normalization, resizing, augmentation).\n",
    "   - Create visualizations of some images, and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Preprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "# Check if GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"GPU Devices: \", tf.config.experimental.list_physical_devices('GPU'))\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "\n",
    "# Cargar CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar muestras\n",
    "num_classes = 10\n",
    "samples_per_class = 10\n",
    "fig, axes = plt.subplots(num_classes, samples_per_class, figsize=(15, 15))\n",
    "\n",
    "for i in range(num_classes):\n",
    "    class_indices = np.where(y_train == i)[0]\n",
    "    random_indices = np.random.choice(class_indices, samples_per_class, replace=False)\n",
    "    for j, idx in enumerate(random_indices):\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(x_train[idx])\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Model Architecture**\n",
    "   - Design a CNN architecture suitable for image classification.\n",
    "   - Include convolutional layers, pooling layers, and fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more Dropout layers and L2 Regularization\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(64, (3,3), padding='same', activation='relu', input_shape=(32,32,3), kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3,3), padding='same', activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Conv2D(128, (3,3), padding='same', activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(128, (3,3), padding='same', activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Conv2D(256, (3,3), padding='same', activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(256, (3,3), padding='same', activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Model Training**\n",
    "   - Train the CNN model using appropriate optimization techniques (e.g., stochastic gradient descent, Adam).\n",
    "   - Utilize techniques such as early stopping to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Learning Rate Scheduler\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 75:\n",
    "        lr = 0.0005\n",
    "    if epoch > 100:\n",
    "        lr = 0.0003\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer,\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Add Early Stopping and Reduce Learning Rate on Plateau\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5)\n",
    "\n",
    "# Fit the model with the new data generator and learning rate scheduler\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=512,\n",
    "                   epochs=150,\n",
    "                   validation_data=(x_test, y_test),\n",
    "                   callbacks=[early_stopping, reduce_lr, lr_scheduler])\n",
    "\n",
    "#plot loss\n",
    "plt.subplot(211)\n",
    "plt.title('Cross Entropy Loss')\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "plt.plot(history.history['val_loss'], color='red', label='val')\n",
    "\n",
    "\n",
    "# plot accuracy\n",
    "plt.subplot(212)\n",
    "plt.title('Classification Accuracy')\n",
    "plt.plot(history.history['accuracy'], color='green', label='train')\n",
    "plt.plot(history.history['val_accuracy'], color='red', label='val')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Model Evaluation**\n",
    "   - Evaluate the trained model on a separate validation set.\n",
    "   - Compute and report metrics such as accuracy, precision, recall, and F1-score.\n",
    "   - Visualize the confusion matrix to understand model performance across different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "# Predicciones\n",
    "predictions = model.predict(x_test)\n",
    "predictions_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Asegurarse de que las clases estén en el rango correcto (0-9 para CIFAR-10)\n",
    "print(\"Rango de predicciones:\", np.min(predictions_classes), \"-\", np.max(predictions_classes))\n",
    "print(\"Rango de valores reales:\", np.min(true_classes), \"-\", np.max(true_classes))\n",
    "\n",
    "# Calcular métricas solo si los rangos son correctos\n",
    "if (0 <= np.min(predictions_classes) <= 9) and (0 <= np.max(predictions_classes) <= 9):\n",
    "    print(\"\\nMatriz de Confusión:\")\n",
    "    print(confusion_matrix(true_classes, predictions_classes))\n",
    "\n",
    "    print(\"\\nMétricas de Evaluación:\")\n",
    "    print(f\"F1-Score: {f1_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "    print(f\"Precisión: {precision_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "    print(f\"Recall: {recall_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "else:\n",
    "    print(\"Error: Las predicciones no están en el rango esperado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_models/img-class-cnn.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Transfer Learning**\n",
    "    - Evaluate the accuracy of your model on a pre-trained models like ImagNet, VGG16, Inception... (pick one an justify your choice)\n",
    "        - You may find this [link](https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub) helpful.\n",
    "        - [This](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) is the Pytorch version.\n",
    "    - Perform transfer learning with your chosen pre-trained models i.e., you will probably try a few and choose the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "# Cargar VGG16 pre-entrenado\n",
    "base_model = VGG16(weights='imagenet', \n",
    "                   include_top=False, \n",
    "                   input_shape=(32, 32, 3))\n",
    "                   #classifier_activation=\"SoftMax\"))\n",
    "\n",
    "# Congelar capas base\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Crear modelo de transfer learning\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "transfer_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Entrenar modelo de transfer learning\n",
    "transfer_model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "history_transfer = transfer_model.fit(x_train, \n",
    "                                    y_train,\n",
    "                                    batch_size=128,\n",
    "                                    epochs=50,\n",
    "                                    validation_data=(x_test, y_test),\n",
    "                                    callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Evaluar métricas después del transfer learning\n",
    "predictions = transfer_model.predict(x_test)\n",
    "predictions_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"\\nMétricas después del transfer learning:\")\n",
    "print(f\"F1-Score: {f1_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "print(f\"Precisión: {precision_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "print(f\"Recall: {recall_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "\n",
    "#plot loss\n",
    "plt.subplot(211)\n",
    "plt.title('Cross Entropy Loss')\n",
    "plt.plot(history_transfer.history['loss'], color='blue', label='train')\n",
    "plt.plot(history_transfer.history['val_loss'], color='red', label='val')\n",
    "\n",
    "\n",
    "# plot accuracy\n",
    "plt.subplot(212)\n",
    "plt.title('Classification Accuracy')\n",
    "plt.plot(history_transfer.history['accuracy'], color='green', label='train')\n",
    "plt.plot(history_transfer.history['val_accuracy'], color='red', label='val')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_models/img-class-cnn-transer.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Comment part 2 of step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning\n",
    "for layer in base_model.layers[-4:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "transfer_model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "history_fine_tuning = transfer_model.fit(x_train, \n",
    "                                       y_train,\n",
    "                                       batch_size=128,\n",
    "                                       epochs=30,\n",
    "                                       validation_data=(x_test, y_test),\n",
    "                                       callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "# Predicciones\n",
    "predictions = transfer_model.predict(x_test)  # Nota: cambiado de base_model a transfer_model\n",
    "predictions_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Asegurarse de que las clases estén en el rango correcto (0-9 para CIFAR-10)\n",
    "print(\"Rango de predicciones:\", np.min(predictions_classes), \"-\", np.max(predictions_classes))\n",
    "print(\"Rango de valores reales:\", np.min(true_classes), \"-\", np.max(true_classes))\n",
    "\n",
    "# Calcular métricas solo si los rangos son correctos\n",
    "if (0 <= np.min(predictions_classes) <= 9) and (0 <= np.max(predictions_classes) <= 9):\n",
    "    print(\"\\nMatriz de Confusión:\")\n",
    "    print(confusion_matrix(true_classes, predictions_classes))\n",
    "\n",
    "    print(\"\\nMétricas de Evaluación:\")\n",
    "    print(f\"F1-Score: {f1_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "    print(f\"Precisión: {precision_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "    print(f\"Recall: {recall_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "else:\n",
    "    print(\"Error: Las predicciones no están en el rango esperado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "#plot loss\n",
    "plt.subplot(211)\n",
    "plt.title('Cross Entropy Loss')\n",
    "plt.plot(history_fine_tuning.history['loss'], color='blue', label='train')\n",
    "plt.plot(history_fine_tuning.history['val_loss'], color='red', label='val')\n",
    "\n",
    "\n",
    "# plot accuracy\n",
    "plt.subplot(212)\n",
    "plt.title('Classification Accuracy')\n",
    "plt.plot(history_fine_tuning.history['accuracy'], color='green', label='train')\n",
    "plt.plot(history_fine_tuning.history['val_accuracy'], color='red', label='val')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_models/img-class-cnn-fine.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
